<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>对似然性的理解</title>
    <url>/2021/03/17/%E5%AF%B9%E4%BC%BC%E7%84%B6%E6%80%A7%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p><strong>似然性与概率</strong></p>
<p>例子：机器学习中已知随机变量D表示数据，随机变量$\theta$表示模型参数。</p>
<p>先验概率$P(\theta)$：已知模型参数$\theta$，也称统计概率。</p>
<p>似然函数$P(D|\theta)$：已知在模型参数$\theta$的条件下，数据为D的概率。</p>
<p>后验概率$P(\theta|D)$：已知数据结果为D，求导致该结果的参数$\theta$，也称条件概率。</p>
<p>贝叶斯公式：即三者关系</p>
<script type="math/tex; mode=display">
P\left( \theta \mid D \right) =\frac{P\left( \theta \right) \cdot P\left( D\mid \theta \right)}{P\left( D \right)}</script><p>预测时，计算数据对后验概率分布的期望值</p>
<script type="math/tex; mode=display">
P\left( x \mid D \right) =\int{P\left( x \mid \theta ,D \right) P\left( \theta \mid D \right) d\theta}</script><p><strong>极大似然估计与贝叶斯（极大后验）估计</strong></p>
<p>极大似然估计（ML）：估计模型的参数，目的是找出一组参数$\theta$，使得模型产生出观测数据D的概率最大。</p>
<script type="math/tex; mode=display">
\hat{\theta}=arg\max_{\theta} P(D|\theta)</script><p>贝叶斯估计：</p>
<script type="math/tex; mode=display">
\hat{P}\left( \theta \mid D \right) =\frac{P\left( \theta \right) \cdot P\left( D\mid \theta \right)}{P\left( D \right)}</script><p>实际应用中可以把两者联系起来，假设先验分布式均匀分布，取后验概率最大，就能从贝叶斯估计中得到极大似然估计。</p>
<hr>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>第一章 统计学习及监督学习概率</tag>
      </tags>
  </entry>
  <entry>
    <title>第一章 深度强化学习概述</title>
    <url>/2021/03/17/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<img src="/2021/03/17/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.png" class title="思维导图">]]></content>
      <categories>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>第一章 深度强化学习概述</tag>
      </tags>
  </entry>
  <entry>
    <title>贝尔曼等式</title>
    <url>/2021/03/19/%E8%B4%9D%E5%B0%94%E6%9B%BC%E7%AD%89%E5%BC%8F/</url>
    <content><![CDATA[<p><strong>Markov Reward Process</strong></p>
<p><strong>Return（回报）</strong>：把奖励进行折扣后所获得的收益。Return 可以定义为奖励的逐步叠加：</p>
<script type="math/tex; mode=display">
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\gamma^3R_{t+4}...</script><p><strong>状态价值函数（state value function）</strong>:Return的期望：</p>
<script type="math/tex; mode=display">
V_t(s)=E[G_t\mid s_t=s]=E[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\gamma^3R_{t+4}...\mid s_t=s]</script><p>所以这个期望也可以看成是对未来可能获得奖励的当前价值的一个表现，就是当你进入某一个状态过后，你现在就有多大的价值。</p>
<p><strong>贝尔曼等式（Bellman Equation）</strong>：定义了当前状态跟未来状态之间的关系。</p>
<p>由简单的数学推到可以得到：</p>
<script type="math/tex; mode=display">
G_t=R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+\gamma^2 R_{t+4}+...)</script><script type="math/tex; mode=display">
迭代：G_t=R_{t+1}+\gamma G_{t+1}</script><p>则贝尔曼等式推到如下：</p>
<script type="math/tex; mode=display">
V(s)=E[G_t\mid s_t=t]\\
=E[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\gamma^3R_{t+4}...\mid s_t=s]\\
=E[R_{t+1}\mid s_t=s]+\gamma E[R_{t+2}+\gamma R_{t+3}+\gamma^2 R_{t+4}+...\mid s_t=s]\\
=E[R_{t+1}+\gamma G_{t+1}\mid s_t=s]\\
=E[R_{t+1}+\gamma\cdot V(s_{t+1})\mid s_t=s]\\
=R(s)+\gamma \sum_{s^,\subseteq S}P(s^,\mid s)V(s^,)</script><p>Bellman Equation 就是当前状态与未来状态的迭代关系，表示当前状态的值函数可以通过下个状态的值函数来计算。</p>
<p>假设当前的状态是考上了好大学，其价值（Value）是拥有一个高品质人生，当前的奖励（Reward）是接受高品质的高等教育，下一个迭代时刻的价值（Value）是拥有一份稳定的工作和报酬。那么<strong>高品质人生 = 接受高品质的高等教育 + 稳定的工作和报酬</strong>，即当前状态跟未来之间的关系。</p>
<p>可以把贝尔曼等式写成一种矩阵的形式：</p>
<img src="/2021/03/19/%E8%B4%9D%E5%B0%94%E6%9B%BC%E7%AD%89%E5%BC%8F/%E8%B4%9D%E5%B0%94%E6%9B%BC%E7%9F%A9%E9%98%B5.png" class title="贝尔曼矩阵">
<script type="math/tex; mode=display">
V=R+\gamma PV\\(1-\gamma P)V=R\\解析解：V=(1-\gamma P)^{-1}R</script><p>我们可以通过矩阵求逆把这个 V 的这个价值直接求出来。但是一个问题是这个矩阵求逆的过程的复杂度是$O(N^3)$。所以当状态非常多的时候，比如说从十个状态到一千个状态，到一百万个状态。那么当我们有一百万个状态的时候，这个转移矩阵就会是个一百万乘以一百万的矩阵，这样一个大矩阵的话求逆是非常困难的，<strong>所以这种通过解析解去求解的方法只适用于很小量的 MRP</strong></p>
<p><strong>Algorithm for Computing Value of a MRP</strong></p>
<img src="/2021/03/19/%E8%B4%9D%E5%B0%94%E6%9B%BC%E7%AD%89%E5%BC%8F/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97.png" class title="蒙特卡罗">
<p>比如说我们要算$s_4$状态的价值。我们可以从$s_4$状态开始，随机产生很多轨迹，就是说产生很多小船，把小船扔到这个转移矩阵里面去，然后它就会随波逐流，产生轨迹。每个轨迹都会得到一个Return，我们得到大量的 Return，比如说一百个、一千个，然后直接取一个平均，那么就可以等价于现在$s_4$这个价值，因为$s_4$的价值$V(s_4)$定义了你未来可能得到多少的奖励。这就是蒙特卡罗采样的方法。</p>
<img src="/2021/03/19/%E8%B4%9D%E5%B0%94%E6%9B%BC%E7%AD%89%E5%BC%8F/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92.png" class title="动态规划">
<p>一直去迭代它的 Bellman equation，让它最后收敛，我们就可以得到它的一个状态。所以在这里算法二就是一个迭代的算法，通过 bootstrapping(拔靴自助) 的办法，然后去不停地迭代这个 Bellman Equation。当这个最后更新的状态跟你上一个状态变化并不大的时候，更新就可以停止，我们就可以输出最新的$V^,(s)$作为它当前的状态。所以这里就是把 Bellman Equation 变成一个 Bellman Update，这样就可以得到它的一个价值。</p>
<p><strong>Markov Decision Process</strong></p>
<p>相对于MRP，马尔可夫决策过程(Markov Decision Process)多了一个decision，其它的定义跟 MRP 都是类似的。</p>
<p>这里多了一个决策，多了一个动作。状态转移也多了一个条件，变成了 $P(s_{t+1}=s^,\mid s_t=s,a_t=a)$。你采取某一种动作，然后你未来的状态会不同。未来的状态不仅是依赖于你当前的状态，也依赖于在当前状态 agent 采取的这个动作。</p>
<p>Ploicy in MDP</p>
<script type="math/tex; mode=display">
\pi(a\mid s)=P(a_t=a\mid s_t=s)</script><img src="/2021/03/19/%E8%B4%9D%E5%B0%94%E6%9B%BC%E7%AD%89%E5%BC%8F/PolicyinMDP.png" class title="PolicyinMDP">
<p>Policy 定义了在某一个状态应该采取什么样的动作，知道当前状态过后，我们可以把当前状态带入 policy function，然后就会得到一个概率，概率就代表了在所有可能的动作里面怎样采取行动，比如可能有 0.7 的概率往左走，有 0.3 的概率往右走，这是一个概率的表示，另外这个策略也可能是确定的，它有可能是直接输出一个值。或者就直接告诉你当前应该采取什么样的动作，而不是一个动作的概率。</p>
<p><strong>Comparison of MP/MRP and MDP</strong></p>
]]></content>
      <categories>
        <category>深度强化学习</category>
      </categories>
      <tags>
        <tag>第二章 马尔可夫决策过程</tag>
      </tags>
  </entry>
</search>
